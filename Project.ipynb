{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Copy of Project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaverioMonaco/VisionProject/blob/master/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cb6de65-d912-46cd-9abc-5b5ff2f959a3"
      },
      "source": [
        "# Image Colorization\n",
        "\n",
        "## Project of Vision And Cognitive Services\n",
        "\n",
        "#### [Saverio Monaco](https://github.com/SaverioMonaco/)\n",
        "#### [Filippo Zillotto](https://github.com/ZiliottoFilippoDev)"
      ],
      "id": "5cb6de65-d912-46cd-9abc-5b5ff2f959a3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e01f8e35-7604-4984-952e-3389efb337f6"
      },
      "source": [
        "# Pytorch functions\n",
        "import torch\n",
        "\n",
        "# Neural network layers\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Optimizer\n",
        "import torch.optim as optim\n",
        "\n",
        "# Handling dataset\n",
        "import torch.utils.data as data\n",
        "\n",
        "# Torchvision library\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "# For results\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import gc\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "id": "e01f8e35-7604-4984-952e-3389efb337f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlD-OBHORhSe"
      },
      "source": [
        "# We don't want any randomness right now\n",
        "SEED = 123\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "id": "hlD-OBHORhSe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiOFe1lcFCqv"
      },
      "source": [
        "# When loading the dataset, this transformation just convert the Image to\n",
        "# grayscale\n",
        "grayscale_transforms = transforms.Compose([torchvision.transforms.Grayscale(num_output_channels=1)])"
      ],
      "id": "aiOFe1lcFCqv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSSeQXTJsHeg"
      },
      "source": [
        "ROOT = './data'\n",
        "\n",
        "data = torchvision.datasets.VOCDetection(root=ROOT,\n",
        "                                          image_set='train',\n",
        "                                          year='2012',\n",
        "                                          download=True)\n",
        "\n",
        "grayscale_data = torchvision.datasets.VOCDetection(root=ROOT,\n",
        "                                          image_set='train',\n",
        "                                          year='2012',\n",
        "                                          download=True,\n",
        "                                         transform=grayscale_transforms)\n"
      ],
      "id": "XSSeQXTJsHeg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkJOn1tUHx0J"
      },
      "source": [
        "''' IMAGES RELATED FUNCTION '''\n",
        "\n",
        "def img_show(num):\n",
        "  gs_img = grayscale_data[0][num] # Load the grayscale image\n",
        "  img = data[0][num]              # Load the colored image\n",
        "\n",
        "  fig = plt.subplots(figsize=(16,6))\n",
        "  \n",
        "  # The title is recovered from a nested dictionary in data[num][1]\n",
        "  plt.suptitle(data[num][1]['annotation']['object'][0]['name'], fontsize=16)\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  # If the channel is just one we need to sed a grayscale image, because \n",
        "  # by default matplotlib set a greenish colour map\n",
        "  plt.imshow(gs_img, cmap='gray')\n",
        "  plt.axis('off')\n",
        "  \n",
        "  "
      ],
      "id": "jkJOn1tUHx0J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glwxlOCzHxZP"
      },
      "source": [
        "img_show(0)"
      ],
      "id": "glwxlOCzHxZP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndk7U6t8PxQN"
      },
      "source": [
        "print('Grayscale Image example:')\n",
        "print('   Shape:',np.shape(np.array(grayscale_data[0][0])[:,:]),'\\n')\n",
        "np.array(grayscale_data[0][0])[:,:]"
      ],
      "id": "ndk7U6t8PxQN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTMCYoO1RxBL"
      },
      "source": [
        "print('Colored Image example:')\n",
        "print('   Shape:',np.shape(np.array(data[0][0])[:,:]),'\\n')\n",
        "np.array(data[0][0])[:,:,0] # Just the first channel, the other 2 are suppressed for printing purposes"
      ],
      "id": "CTMCYoO1RxBL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ds9n-ViPgva"
      },
      "source": [
        ""
      ],
      "id": "1ds9n-ViPgva",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "f04bdc5a-9787-4fe5-81bd-22ab22c3de78"
      },
      "source": [
        "#### References:\n",
        "* https://richzhang.github.io/colorization/\n",
        "* https://arxiv.org/abs/1603.08511\n",
        "* https://arxiv.org/abs/1908.01311"
      ],
      "id": "f04bdc5a-9787-4fe5-81bd-22ab22c3de78"
    }
  ]
}